# AI CLI â€” Interface de Commande Neuro-Symbolique

**Package :** `ai_cli`
**Localisation :** `src-tauri/tools/ai_cli`
**RÃ´le :** Outil de dÃ©veloppement, de debug et d'automatisation pour le backend IA de GenAptitude.

---

## ğŸ¯ Objectifs

L'`ai_cli` est un exÃ©cutable lÃ©ger ("Thin Client") qui permet d'interagir directement avec la librairie `genaptitude` sans passer par l'interface graphique (Tauri/React).

Il est essentiel pour :

1.  **Tester la "plomberie"** : VÃ©rifier que le LLM Local (Mistral) ou Cloud (Gemini) rÃ©pond bien.
2.  **Valider le NLU** : S'assurer que le `IntentClassifier` comprend bien les phrases de l'ingÃ©nieur.
3.  **ExÃ©cuter des Agents** : Lancer des tÃ¢ches de modÃ©lisation sur **toutes les couches** (OA -> EPBS + Data + Transverse) directement depuis le terminal.

---

## ğŸ›ï¸ Architecture & Flux

L'outil instancie les mÃªmes structures que le serveur Tauri (`LlmClient`, `StorageEngine`, `AgentContext`) mais dans un environnement CLI Ã©phÃ©mÃ¨re.

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEVELOPER   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ cargo run -p ai_cli -- classify "..."
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        1. Identification         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AI_CLI BINARY     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ INTENT CLASSIFIER â”‚
â”‚ (src-tauri/tools...) â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ (Rules + LLM)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        2. EngineeringIntent      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ 3. Dispatch (Business, System, Data, etc.)
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     AGENT SQUAD      â”‚        4. Enrichissement         â”‚    LLM BACKEND    â”‚
    â”‚ (OA/SA/LA/PA/EPBS..) â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  (Local / Cloud)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                    5. JSON Content
               â”‚
               â”‚ 6. Persistance (Si flag -x)
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   STORAGE ENGINE     â”‚
    â”‚ (Filesystem JSON-DB) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

## âš™ï¸ Configuration

L'outil charge automatiquement le fichier `.env` situÃ© Ã  la racine du monorepo (`../../../../.env`).

**Variables Indispensables :**

```bash
# Choix du mode (Hybrid)
GENAPTITUDE_MODE_DUAL="true"

# URLs & ClÃ©s
GENAPTITUDE_LOCAL_URL="http://localhost:8080"
GENAPTITUDE_GEMINI_KEY="AIza..."

# Cibles de donnÃ©es (Absolues de prÃ©fÃ©rence)
PATH_GENAPTITUDE_DOMAIN="/home/user/genaptitude/data"
PATH_GENAPTITUDE_DATASET="/home/user/genaptitude/dataset"

```

---

## ğŸš€ Commandes Disponibles

### 1. `chat` (Mode Conversationnel)

Permet de tester la latence et la rÃ©ponse brute du LLM (Mode RAG simulÃ©).

**Syntaxe :**

```bash
cargo run -p ai_cli -- chat [OPTIONS] <MESSAGE>

```

**Options :**

- `-c, --cloud` : Force l'utilisation de Google Gemini (si configurÃ©). Sinon, utilise LocalLlama.

**Exemple :**

```bash
cargo run -p ai_cli -- chat "Quelle est la diffÃ©rence entre OA et SA dans Arcadia ?"

```

---

### 2. `classify` (Mode IngÃ©nierie)

C'est la commande principale. Elle simule le cycle complet : **Intention -> Agent -> DB**.

**Syntaxe :**

```bash
cargo run -p ai_cli -- classify [OPTIONS] <INPUT>

```

**Options :**

- `-x, --execute` : **Active l'Ã©criture**. Sans ce flag, l'outil tourne en mode "Dry Run" (simulation) : il affiche l'intention dÃ©tectÃ©e et l'agent qui _serait_ appelÃ©, mais ne touche pas Ã  la base de donnÃ©es.

**ScÃ©narios supportÃ©s (Couverture ComplÃ¨te) :**

| Couche             | Intention DÃ©tectÃ©e           | Exemple de commande                           |
| ------------------ | ---------------------------- | --------------------------------------------- |
| **OA** (Business)  | `DefineBusinessUseCase`      | `"Je veux gÃ©rer les congÃ©s payÃ©s RH"`         |
| **SA** (SystÃ¨me)   | `CreateElement` (SA)         | `"CrÃ©e une fonction systÃ¨me DÃ©marrer Moteur"` |
| **LA** (Logiciel)  | `CreateElement` (LA)         | `"CrÃ©e un composant AuthService"`             |
| **PA** (MatÃ©riel)  | `CreateElement` (PA)         | `"CrÃ©e un serveur Rack Dell R750"`            |
| **EPBS** (Config)  | `CreateElement` (EPBS)       | `"Ajoute un CI pour la carte mÃ¨re"`           |
| **DATA** (DonnÃ©es) | `CreateElement` (DATA)       | `"DÃ©fini la classe Client avec nom et email"` |
| **TRANSVERSE**     | `CreateElement` (TRANSVERSE) | `"Ajoute une exigence de performance"`        |
| **CODE**           | `GenerateCode`               | `"GÃ©nÃ¨re le code Rust pour une API REST"`     |

**Exemple Complet (Hardware) :**

```bash
# 1. Simulation (Dry Run)
cargo run -p ai_cli -- classify "CrÃ©e un FPGA Xilinx pour le traitement vidÃ©o"

# Sortie :
# ğŸ§  Analyse de l'intention...
# ğŸ”§ Intention Hardware dÃ©tectÃ©e (PA) -> HardwareAgent
# (Mode Dry Run - Utilisez -x pour exÃ©cuter rÃ©ellement)

# 2. ExÃ©cution RÃ©elle
cargo run -p ai_cli -- classify "CrÃ©e un FPGA Xilinx pour le traitement vidÃ©o" -x

# Sortie :
# ğŸ§  Analyse...
# ğŸ”§ ExÃ©cution Hardware Agent (PA)...
# âœ… SUCCÃˆS :
# [Hardware] Electronics crÃ©Ã© : VideoProcessingUnit (ID: ...)

```

---

## ğŸ› DÃ©pannage

| Erreur               | Solution                                                                 |
| -------------------- | ------------------------------------------------------------------------ |
| `Connection refused` | VÃ©rifiez que votre serveur local (Ollama/Llama) tourne sur le port 8080. |
| `API Key Missing`    | VÃ©rifiez votre fichier `.env`.                                           |
| `Partial move`       | Erreur de compilation Rust interne (signalez-le Ã  l'Ã©quipe).             |
| `Schema not found`   | Le dossier `dataset` est mal configurÃ© dans le `.env`.                   |

---

> **Note DÃ©veloppeur :** Pour voir les logs dÃ©taillÃ©s (requÃªtes HTTP, parsing JSON), utilisez :
> `RUST_LOG=debug cargo run -p ai_cli ...`

```

```
