# Module Low-Level LLM üß†

Ce module est la **couche d'abstraction bas niveau** responsable de la communication avec les mod√®les d'intelligence artificielle.
Il isole le reste de l'application Rust de la complexit√© des APIs tierces (OpenAI format, Google REST API) et assure la r√©silience du service.

---

## üèóÔ∏è Architecture & Flux de Donn√©es

Le client impl√©mente un pattern **"Smart Fallback"**. Il tente toujours de privil√©gier l'inf√©rence locale (confidentialit√©, co√ªt) mais bascule automatiquement et silencieusement vers le Cloud en cas d'indisponibilit√©.

```text
    [ Application Rust (Agents) ]
                 |
                 v
      +---------------------+
      |      LlmClient      |
      | (Interface Unifi√©e) |
      +---------------------+
                 |
        1. Tentative LOCAL
                 |
                 v
      /---------------------\
      |   API Locale (HTTP) | <--- Ping / Timeout (2s)
      \---------------------/
         |             |
     [Succ√®s]      [√âchec / 404]
         |             |
         |             v
         |     2. Bascule CLOUD (Fallback)
         |             |
         |             v
         |    /-----------------\
         |    |  Google Gemini  | (REST API v1beta)
         |    \-----------------/
         |             |
         |             |
         v             v
      +---------------------+
      |   R√©ponse Textuelle |
      +---------------------+
```

---

## üìÇ Structure des Fichiers

| Fichier              | Responsabilit√©                                                                                                                                        |
| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **`client.rs`**      | **C≈ìur du module**. Impl√©mente `LlmClient`, la gestion HTTP (reqwest), la logique de fallback et le nettoyage des noms de mod√®les (`models/` prefix). |
| `response_parser.rs` | _(Utilitaire)_ Fonctions pour extraire et valider le JSON depuis les blocs de code Markdown (```json) renvoy√©s par les LLMs.                          |
| `prompts.rs`         | _(Utilitaire)_ Biblioth√®que de prompts syst√®me (System Prompts) pour sp√©cialiser l'IA (Expert Rust, Expert SQL, Architecte Arcadia).                  |
| `mod.rs`             | Point d'entr√©e du module, expose les types publics.                                                                                                   |
| `tests.rs`           | Tests d'int√©gration pour v√©rifier la connexion aux backends (Local et Cloud) et le parsing.                                                           |

---

## üöÄ Fonctionnalit√©s Cl√©s

### 1. Smart Fallback (R√©silience)

Le syst√®me est con√ßu pour le d√©veloppement hybride :

- **Mode Local (`LocalLlama`)** : Cible par d√©faut (ex: `localhost:8080/v1/...`). Id√©al pour le dev hors-ligne ou la confidentialit√©.
- **Mode Cloud (`GoogleGemini`)** : S'active si le serveur local ne r√©pond pas sous 2 secondes. Utilise l'API Google Generative Language.

### 2. Normalisation des Mod√®les Gemini

Le client g√®re automatiquement les incoh√©rences de nommage de l'API Google.

- Entr√©e config : `models/gemini-1.5-flash` ou `gemini-1.5-flash`
- Traitement interne : Nettoie le pr√©fixe `models/` pour construire une URL API valide (`.../models/gemini-1.5-flash:generateContent`).

### 3. Typage Fort

Utilise des structures Rust (`struct`) pour s√©rialiser/d√©s√©rialiser proprement les requ√™tes JSON, garantissant que les payloads envoy√©s √† OpenAI ou Google sont toujours conformes.

---

## ‚öôÔ∏è Configuration

Le client est instanci√© avec des param√®tres provenant g√©n√©ralement des variables d'environnement (`.env`) charg√©es par le binaire principal.

| Variable Env             | Usage                                                              |
| ------------------------ | ------------------------------------------------------------------ |
| `GENAPTITUDE_MODEL_NAME` | Nom du mod√®le (ex: `gemini-2.0-flash-001`). Le pr√©fixe est g√©r√©.   |
| `GENAPTITUDE_GEMINI_KEY` | Cl√© API Google (commence par `AIza...`).                           |
| `GENAPTITUDE_LOCAL_URL`  | URL du serveur d'inf√©rence local (ex: `http://localhost:1234/v1`). |

---

## üíª Exemple d'Utilisation (Rust)

```rust
use crate::ai::llm::client::{LlmClient, LlmBackend};

async fn example() {
    // 1. Instanciation
    let client = LlmClient::new(
        "http://localhost:1234",
        "AIzaSy...",
        Some("gemini-1.5-flash".to_string())
    );

    // 2. Appel (Le fallback est g√©r√© en interne si LocalLlama est choisi)
    let reponse = client.ask(
        LlmBackend::LocalLlama, // Tente le local d'abord
        "Tu es un expert Rust.", // System Prompt
        "G√©n√®re une struct Client." // User Prompt
    ).await;

    match reponse {
        Ok(text) => println!("R√©ponse IA : {}", text),
        Err(e) => eprintln!("Erreur critique : {}", e),
    }
}

```
