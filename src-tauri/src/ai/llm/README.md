# Module `ai::llm` - Infrastructure Bas Niveau LLM

Ce module constitue la couche d'infrastructure (**Low-Level Layer**) de GenAptitude pour la communication avec les mod√®les de langage. Il fournit la "tuyauterie" technique permettant aux Agents de fonctionner sans se soucier de la complexit√© r√©seau ou du formatage des r√©ponses.

---

## üìÇ Structure du Module

Voici l'organisation physique des fichiers de ce module :

```text
src-tauri/src/ai/llm/
‚îú‚îÄ‚îÄ mod.rs               # Point d'entr√©e : expose les sous-modules publics.
‚îú‚îÄ‚îÄ client.rs            # Client HTTP : g√®re la connexion (Ollama/Gemini) et le Fallback.
‚îú‚îÄ‚îÄ prompts.rs           # Personas : contient les constantes des "System Prompts".
‚îú‚îÄ‚îÄ response_parser.rs   # Nettoyeur : extrait le JSON/Code des r√©ponses brutes.
‚îî‚îÄ‚îÄ tests.rs             # Validation : tests unitaires et d'int√©gration.

```

---

## üìä Architecture & Flux de Donn√©es

Le syst√®me impl√©mente une strat√©gie **"Local First"** avec un m√©canisme de **Nettoyage Automatique** des r√©ponses.

### Sch√©ma du Flux (Pipeline)

```text
    +-----------+                                     +-----------------+
    |   AGENT   |  >> 1. Envoi du Prompt (Persona) >> |   LLM CLIENT    |
    +-----------+                                     +-----------------+
          ^                                                    |
          |                                          (Tentative Local : OLLAMA)
          |                                                    v
    (Retour JSON)                                    [ ECHEC ? -> FALLBACK ]
          |                                                    |
          |                                           (Tentative Cloud : GEMINI)
          |                                                    |
    +-----------+                                              |
    |   PARSER  |  << 3. Nettoyage (No Markdown) <<   (R√©ponse Brute)
    +-----------+

```

### Description des √âtapes

1. **Conditionnement (`prompts.rs`) :** L'Agent s√©lectionne une personnalit√© (ex: `SYSTEM_AGENT_PROMPT`) pour orienter l'expertise du mod√®le.
2. **Transport & R√©silience (`client.rs`) :**

- Le client tente d'abord d'interroger le mod√®le local (port 11434 ou 8080).
- Si le serveur local ne r√©pond pas, il bascule automatiquement sur l'API Google Gemini (si la cl√© est configur√©e).

3. **Nettoyage (`response_parser.rs`) :**

- La r√©ponse brute arrive souvent pollu√©e (ex: "Voici le JSON : `json ... `").
- Le parser extrait chirurgicalement les donn√©es utiles (JSON ou Code) avant de les renvoyer √† l'Agent.

---

## üíª Exemples d'Utilisation (Rust)

Voici comment utiliser les briques de ce module pour construire un Agent.

### Cas 1 : Analyse d'Intention (Retour JSON)

Ce cas est utilis√© par le `IntentClassifier` pour router la demande.

````rust
use crate::ai::llm::{client, prompts, response_parser};

async fn classify_user_request(user_input: &str) -> Result<serde_json::Value, String> {
    // 1. Initialisation du Client (souvent fait au d√©marrage de l'app)
    // On cible le port par d√©faut d'Ollama
    let llm_client = client::LlmClient::new("http://localhost:11434", "optional_api_key", None);

    // 2. Construction du Prompt avec le Persona "Routeur"
    let full_prompt = format!(
        "{}\n\nUSER REQUEST: {}",
        prompts::INTENT_CLASSIFIER_PROMPT,
        user_input
    );

    // 3. Envoi de la requ√™te (Le client g√®re le r√©seau et le fallback)
    let raw_response = llm_client.ask_raw(&full_prompt).await
        .map_err(|e| format!("Erreur LLM: {}", e))?;

    // 4. Nettoyage et Parsing JSON
    // Cela g√®re les cas o√π l'IA r√©pond "Voici le JSON : ```json { ... } ```"
    let json_data = response_parser::extract_json(&raw_response)
        .map_err(|e| format!("Erreur Parsing: {}", e))?;

    // On retourne l'objet JSON propre
    Ok(json_data)
}

````

### Cas 2 : G√©n√©ration de Code (Retour Texte Brut)

Ce cas est utilis√© par le `SoftwareAgent` pour √©crire des fichiers Rust.

````rust
use crate::ai::llm::{client, prompts, response_parser};

async fn generate_rust_code(task_description: &str) -> Result<String, String> {
    let llm_client = client::LlmClient::new("http://localhost:11434", "", None);

    // On utilise le Persona "Software Engineer"
    let prompt = format!("{}\nTask: {}", prompts::SOFTWARE_AGENT_PROMPT, task_description);

    let raw_response = llm_client.ask_raw(&prompt).await
        .map_err(|e| e.to_string())?;

    // Ici, on ne veut pas parser du JSON, mais extraire le bloc de code
    // Cette fonction retire le texte "Voici le code" et les balises ```rust
    let clean_code = response_parser::extract_code_block(&raw_response);

    Ok(clean_code)
}

````

---

## ‚öôÔ∏è Configuration Requise

Variables d'environnement (fichier `.env` ou contexte d'ex√©cution) :

| Variable                    | Description                                                 |
| --------------------------- | ----------------------------------------------------------- |
| `GENAPTITUDE_LLM_LOCAL_URL` | URL du serveur local (d√©faut : `http://localhost:11434/v1`) |
| `GENAPTITUDE_GEMINI_KEY`    | Cl√© API de secours (Google AI Studio)                       |

---

## ‚úÖ Validation

Pour v√©rifier que ce module fonctionne correctement (Parser + Prompts + Client), ex√©cutez la suite de tests d√©di√©e :

```bash
cargo test ai::llm

```
