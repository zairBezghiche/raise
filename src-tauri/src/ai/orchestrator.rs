use crate::ai::context::{
    conversation_manager::ConversationSession, memory_store::MemoryStore, rag::RagRetriever,
    retriever::SimpleRetriever,
};
use crate::ai::llm::client::{LlmBackend, LlmClient};
use crate::model_engine::types::ProjectModel;
use anyhow::{Context, Result};
use std::env;
use std::path::PathBuf;

pub struct AiOrchestrator {
    rag: RagRetriever,
    symbolic: SimpleRetriever,
    llm: LlmClient,
    // Nouveaux composants M√©moire
    session: ConversationSession,
    memory_store: MemoryStore,
}

impl AiOrchestrator {
    /// Initialise l'orchestrateur.
    /// Charge automatiquement la session "default_session" (pour l'instant).
    pub async fn new(model: ProjectModel, qdrant_url: &str, llm_url: &str) -> Result<Self> {
        // 1. Init des moteurs de recherche
        let rag = RagRetriever::new(qdrant_url).await?;
        let symbolic = SimpleRetriever::new(model);
        let llm = LlmClient::new(llm_url, "", None);

        // 2. Init de la Persistance (Bas√©e sur le .env)
        // On cherche le chemin de stockage d√©fini dans le .env, ou on utilise un d√©faut
        let domain_path = env::var("PATH_GENAPTITUDE_DOMAIN")
            .unwrap_or_else(|_| ".genaptitude_storage".to_string());

        let chats_path = PathBuf::from(domain_path).join("chats");
        let memory_store = MemoryStore::new(&chats_path)
            .context("Impossible d'initialiser le stockage des chats")?;

        // 3. Chargement de la session (Id fixe pour le moment : 'main_session')
        // Dans le futur, cet ID viendra de l'UI.
        let session_id = "main_session";
        let session = memory_store.load_or_create(session_id)?;

        Ok(Self {
            rag,
            symbolic,
            llm,
            session,
            memory_store,
        })
    }

    /// Pr√©pare le contexte complet : Historique + Mod√®le + RAG
    async fn prepare_prompt(&mut self, query: &str) -> Result<String> {
        // 1. Recherche RAG & Symbolique
        let rag_context = self.rag.retrieve(query, 3).await?;
        let symbolic_context = self.symbolic.retrieve_context(query);

        // 2. R√©cup√©ration de l'historique conversationnel
        let history_context = self.session.to_context_string();

        // 3. Construction du Prompt Syst√®me Unique
        let mut prompt = String::from(
            "Tu es l'assistant intelligent de GenAptitude (Expert Syst√®me Arcadia).\n\
             R√©ponds √† la question de l'ing√©nieur en utilisant le contexte ci-dessous.\n\
             Si l'utilisateur fait r√©f√©rence √† '√ßa', 'il' ou 'le', regarde l'HISTORIQUE.\n\n",
        );

        // Injection des blocs (seulement si non vides pour √©conomiser des tokens)
        if !history_context.is_empty() {
            prompt.push_str(&history_context);
        }

        if !symbolic_context.is_empty() {
            prompt.push_str("### MOD√àLE SYST√àME (V√©rit√© Terrain) ###\n");
            prompt.push_str(&symbolic_context);
            prompt.push_str("\n\n");
        }

        if !rag_context.is_empty() {
            prompt.push_str("### DOCUMENTATION (Connaissance RAG) ###\n");
            prompt.push_str(&rag_context);
            prompt.push_str("\n\n");
        }

        prompt.push_str("### NOUVELLE QUESTION ###\n");
        prompt.push_str(query);

        Ok(prompt)
    }

    /// La m√©thode principale : Traite la question, met √† jour la m√©moire et r√©pond.
    pub async fn ask(&mut self, query: &str) -> Result<String> {
        // A. On ajoute la question √† la m√©moire court-terme
        self.session.add_user_message(query);

        // B. On pr√©pare le prompt g√©ant
        let prompt = self.prepare_prompt(query).await?;

        println!("üó£Ô∏è Envoi au LLM ({} chars)...", prompt.len());

        // C. Appel LLM
        let response = self
            .llm
            .ask(LlmBackend::LlamaCpp, "Tu es un expert.", &prompt)
            .await
            .map_err(|e| anyhow::anyhow!("Erreur LLM: {}", e))?;

        // D. On sauvegarde la r√©ponse et on persiste sur disque
        self.session.add_ai_message(&response);
        self.memory_store.save_session(&self.session)?;

        Ok(response)
    }

    pub async fn learn_document(&mut self, content: &str, source: &str) -> Result<()> {
        self.rag.index_document(content, source).await
    }

    /// (Pour le debug) R√©initialise la conversation
    pub fn clear_history(&mut self) -> Result<()> {
        self.session = ConversationSession::new(self.session.id.clone());
        self.memory_store.save_session(&self.session)?;
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::model_engine::types::{ArcadiaElement, NameType, ProjectModel};
    use serde_json::json;
    use std::collections::HashMap;
    use std::env;
    use std::time::Duration;

    fn create_mock_model() -> ProjectModel {
        let mut model = ProjectModel::default();
        let drone = ArcadiaElement {
            id: "uuid-drone-123".to_string(),
            name: NameType::String("Drone de Livraison".to_string()),
            kind: "http://genaptitude.io/ontology/oa#OperationalActor".to_string(),
            properties: HashMap::from([(
                "description".to_string(),
                json!("Acteur principal du syst√®me"),
            )]),
        };
        model.oa.actors.push(drone);
        model
    }

    #[tokio::test]
    async fn test_conversation_memory() {
        // 1. CONFIGURATION VIA .ENV
        dotenvy::dotenv().expect("‚ùå .env manquant");
        let llm_url = env::var("GENAPTITUDE_LOCAL_URL").expect("GENAPTITUDE_LOCAL_URL manquant");
        let qdrant_port = env::var("PORT_QDRANT_GRPC").expect("PORT_QDRANT_GRPC manquant");

        // On force 127.0.0.1 pour Qdrant URL
        let qdrant_url = format!("http://127.0.0.1:{}", qdrant_port);

        // Health check rapide
        let client = reqwest::Client::new();
        let health_url = format!("{}/health", llm_url.trim_end_matches('/'));
        if client
            .get(&health_url)
            .timeout(Duration::from_secs(2))
            .send()
            .await
            .is_err()
        {
            println!("‚ö†Ô∏è TEST IGNOR√â : LLM √©teint.");
            return;
        }

        // 2. INIT
        let model = create_mock_model();
        let mut orchestrator = AiOrchestrator::new(model, &qdrant_url, &llm_url)
            .await
            .expect("Init failed");

        // On nettoie l'historique pour le test
        orchestrator.clear_history().unwrap();

        // 3. TOUR 1 : Injection d'information dans la conversation
        println!("üí¨ Tour 1 : D√©finition du sujet");
        let query1 = "Je travaille sur le projet secret 'Zeus'. C'est un satellite m√©t√©o.";
        let rep1 = orchestrator.ask(query1).await.expect("Fail Turn 1");
        println!("ü§ñ IA: {}", rep1);

        // 4. TOUR 2 : Question contextuelle (R√©f√©rence anaphorique)
        println!("üí¨ Tour 2 : Question m√©moire");
        // Ici, l'IA ne peut r√©pondre QUE si elle se souvient du Tour 1
        let query2 = "Quel est le but de ce projet secret ?";
        let rep2 = orchestrator.ask(query2).await.expect("Fail Turn 2");
        println!("ü§ñ IA: {}", rep2);

        // 5. VALIDATION
        let rep2_lower = rep2.to_lowercase();
        assert!(
            rep2_lower.contains("m√©t√©o") || rep2_lower.contains("satellite"),
            "‚ùå L'IA a oubli√© le contexte de la conversation !"
        );

        println!("‚úÖ SUCC√àS : L'Orchestrateur a de la m√©moire !");
    }
}
